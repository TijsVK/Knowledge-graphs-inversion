%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER                              %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\chapter{Conclusion}
\label{chapter:conclusion}
In this master thesis we explored the possibility of inverting knowledge graphs back to their original raw data format using mapping rules. To this end we created a generic two-module design and made an implementation using RML mapping rules. We implemented a data retrieval module using SPARQL and two templating modules for CSV and JSON. 

The data retrieval module takes the knowledge graph, either a file or an endpoint, and uses the mapping rules to retrieve the data from it. In our case it translates selected triple maps in the mapping rules to parts of a SPARQL query, adapting the approach based on the type of the triple map. The selection process aims to balance performance and reliability. Generating the output is made more challenging by the requirement to handle both absent fields and multiple occurrences of the same field while ensuring their equality. We found that to ensure proper inversion we cannot use the full RML specification, but must impose restrictions on the mapping rules. These restrictions pertain to the access of the source and transformation of the data. 
To fully reconstruct a data source no aggregation, filtering or other irreversible transformations on the source can be done. Additionally, the mapping must use all the data in the source. 
In order to be able to reconstruct values of the data, no possibly irreversible transformations can be done on them, as any information lost during a lossy transformation is impossible to regain. This includes the use of templates, where the divider between different references cannot be present in the references. \acrshort{uri} encoded templates are an exception, here the transformation is guaranteed to be reversible if reserved characters are used as divider.

The templating module takes the data retrieved by the data retrieval module and uses it to create the source file. In case of the CSV templating module this is a simple process, as the data is already in the correct format. The JSON templating module is more complex, as JSON has a nested structure. We take all the paths from which data is retrieved and create a template from them. This template is then recursively filled from the root node, splitting the data at each array. To properly create the JSON template all filepaths must be linearly connected to the root node, using JSON features like recursive descent makes the path taken to the data unclear and the template impossible to create. This is another restriction we must impose on the mapping rules.

We evaluated our implementation using the RML test cases to test various edge cases, the LUBM4OBDA benchmark to test the scalability of our data retrieval module and the GTFS-Madrid benchmark to evaluate the performance of the JSON templating module. We find that our implementation is able to invert the knowledge graph back to the original source files in most cases. The failures are mostly due to limitations of the mapping processor, incompatible mapping rules or different data representations. The LUBM4OBDA benchmark shows that our data retrieval module scales linearly with the size of the knowledge graph and is completely dependent on the power of the SPARQL endpoint. The GTFS-Madrid benchmark shows that the JSON templating module also scales linearly with the size of the knowledge graph.

\section{Future work}
In this section we will discuss possible future work that can be done to expand on our implementation. The implementation can be expanded on in both depth and breadth. 

\subsection{Functions}
RML offers the capability to transform data using functions with the Function Ontology (FnO). Expanding the capabilities of the data retrieval module to encompass this is a possible next step. Functions that merely transform data without losing information could be directly inverted. The data lost in lossy transformations could in some cases be regained by calling on an external data source. Lastly lossy functions could be used to verify the correctness of data values with duplicate references of which one was run through a lossy function.

Implementing this would expand the depth of the implementation. We did not implement it due to the large complexity and time to implement it compared to the limited benefits it is expected to provide.

\subsection{Additional templating modules}
We created two templating modules, one for CSV and one for JSON. Each a representative of a different type of data format: tabular and nested. Creating additional templating modules would make the implementation more versatile, further allowing mixed data formats in mappings. 

A special case where we don't recreate the data source would be to make a database update module. If the primary key of a table is present in the references an update query could be made to update the database. 