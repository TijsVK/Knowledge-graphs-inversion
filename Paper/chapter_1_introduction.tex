%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%                                                                 %
%                            CHAPTER                              %
%                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\chapter{Introduction}
\label{chapter:introduction}

The earliest academic definition of a knowledge graph can be found in a 1974 article as \begin{quote}
    A mathematical structure with vertices as knowledge units connected by edges that represent the prerequisite relation \citep{Marchi1974,bergman2019common}
\end{quote} 

The idea of expressing knowledge in a graph structure predates even this definition, with the concept of semantic networks \citep{Richens1956PreprogrammingFM}. % this is in the ago of punch card computers, so quite impressive 
However, the term knowledge graph only became well-known after Google announced they were using a knowledge graph to enhance their search engine in 2012 \citep{singhal2012introducing}. 
Knowledge graphs are used to make search engines, chatbots, question answering systems, etc more intelligent by injecting knowledge into them \citep{SurveyOnKGs}. 

A knowledge graph consists of many connected nodes, where each node is either an entity or a literal. These nodes are connected by edges, where each edge defines a relation between two nodes. \acrshort{rdf} is a framework often used to represent knowledge graphs, it uses subject-predicate-object triples to represent the nodes and their edges. Every node is either an \acrshort{uri}, a blank node or a literal. The edges are \acrshortpl{uri}. A triple representing the fact that the entity \texttt{John Doe} has the first name \texttt{John} would look like this: \texttt{http://example.com/John\_Doe http://schema.org/givenName "John"}. Often the predicates are chosen from an ontology/vocabulary, such as schema.org or \acrshort{foaf}. This allows for more interoperability between knowledge graphs, as the same predicates are used to represent the same concepts.

These knowledge graphs are constructed by extracting information from various sources, both unstructured sources such as text (using natural language processing) and (semi-)structured sources such as databases, CSV, XML, JSON, RDF (using mapping languages). Many mapping languages exist, differing on the way of defining the rules and the target source file format. Some mapping languages use the turtle syntax, while others provide their own custom syntax, and others repurpose existing languages like \acrshort{sparql} or \acrshort{shex}. \citep{VANASSCHE2023100753}. Some languages are specific to a single source format, such as R2RML(turtle format) \citep{Das:12:RRR} for relational databases, XSPARQL(\acrshort{sparql} format) \citep{Bischof2012} for XML. Others can process multiple formats, such as RML (turtle) \citep{dimou_ldow_2014}, D-REPR (\acrshort{yaml}), xR2RML (turtle), etc. These have the ability to map from multiple sources in different formats.

To achieve this these mapping languages use a declarative approach, where the user specifies the mapping rules, and the implementation of the mapping language takes care of the actual mapping. Two ways of doing the mapping exist: materialisation and virtualisation. Materialisation constructs the knowledge graph as a file, which can be loaded into a triple store. Virtualisation does not generate the knowledge graph as a file, but instead exposes a virtual knowledge graph, which can be queried as if it were a real knowledge graph. \citep{ontop}.

Creating these mapping rules is often done by hand. There are tools that make creating these mappings easier, like RMLEditor \citep{heyvaert_jws_2018} which exposes a visual editor and YARRRML \citep{10.1007/978-3-319-98192-5_40} which allows users to create rules in the user-friendly \acrshort{yaml} which are then compiled to RML rules. Alternatively tools are starting to be created for automatic generation of mapping rules from e.g. \acrshort{shacl}.
% TODO: do this citing

Retrieving data from a knowledge graph, for consumption by other programs, is done by querying the knowledge graph using SPARQL \citep{Seaborne:08:SQL} for tabular data and XSPARQL \citep{Bischof2012} or XSLT for XML. XSPARQL is the only language that can both map[/lift] and query[/lower], but the syntax for mapping and querying differs, so it could be argued that XSPARQL is actually two languages.

We can not convert the knowledge graph back to the original data format using the same rules we created it with. As such any changes we make to the data are hard to propagate back to the original data. We can not update, expand or improve the original data using e.g. knowledge graph refining. Nor can we apply changes to a virtual knowledge graph to change the original data. 

In thesis we seek to answer the question: \textit{How can we extend an existing system like RML or create a new system to construct
raw data from knowledge graphs?} We choose to extend the Morph-KGC implementation \citep{arenas2022morph} of \acrshort{rml} \citep{dimou_ldow_2014} as \acrshort{rml}'s end-to-end (from file to knowledge graph) characteristics make it a good candidate for this task. To answer the main research question we need to answer the following sub-questions:
\begin{itemize}
    \item[\textit{RQ1}] \textit{How can we construct the schema of the original data from the mapping rules?}
    \item[\textit{RQ2}] \textit{How can we populate the schema with data from the knowledge graph?}
\end{itemize}

\section{Thesis outline}
This thesis aims to explore the possibility of inverting knowledge graphs back to their original data format using RML mapping rules. To achieve this we will first look at the current state of the art in chapter \ref{chapter:related_work}. We will take a closer look at the technologies used like RDF, SPARQL, and RML. We will also look at the current state of the art for inverting knowledge graphs. In chapter \ref{chapter:implementation} we will look at our implementation of the inversion algorithm. We will look at the algorithm itself, and the implementation details. In chapter \ref{chapter:evaluation} we will evaluate our implementation using various benchmarks. For basic testing we use a subset of the rml test cases, which are designed to test the conformance of tools to the RML specification. For more advanced testing we will use various benchmarks simulating real-life use cases like LUBM4OBDA, GTFS-Madrid-Bench and SDM-Genomic-dataset. Finally in chapter \ref{chapter:conclusion} we will conclude this thesis, and look at possible future work.
